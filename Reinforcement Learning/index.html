<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Reinforcement Learning/Reinforcement Learning">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.1.0">
<title data-rh="true">Reinforcement Learning | My Digital Garden</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ashok-arora.github.io/digital-garden/Reinforcement Learning/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Reinforcement Learning | My Digital Garden"><meta data-rh="true" name="description" content="Any goal can be formalized as the outcome of maximizing a cumulative reward."><meta data-rh="true" property="og:description" content="Any goal can be formalized as the outcome of maximizing a cumulative reward."><link data-rh="true" rel="icon" href="/digital-garden/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ashok-arora.github.io/digital-garden/Reinforcement Learning/"><link data-rh="true" rel="alternate" href="https://ashok-arora.github.io/digital-garden/Reinforcement Learning/" hreflang="en"><link data-rh="true" rel="alternate" href="https://ashok-arora.github.io/digital-garden/Reinforcement Learning/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://DWN3V7NVH5-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="My Digital Garden" href="/digital-garden/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/digital-garden/assets/css/styles.14edd167.css">
<link rel="preload" href="/digital-garden/assets/js/runtime~main.87528bd9.js" as="script">
<link rel="preload" href="/digital-garden/assets/js/main.dcaed46e.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="theme.common.skipToMainContent"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/digital-garden/"><div class="navbar__logo"><img src="/digital-garden/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/digital-garden/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">My Digital Garden</b></a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/digital-garden/">Hello World!</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" aria-expanded="true" href="/digital-garden/Reinforcement Learning/">Reinforcement Learning</a><button aria-label="Toggle the collapsible sidebar category &#x27;Reinforcement Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/digital-garden/Reinforcement Learning/Cartpole Problem">Cartpole Problem</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/digital-garden/Reinforcement Learning/Neural Networks/CNN">Neural Networks</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/digital-garden/DevOps/">DevOps</a><button aria-label="Toggle the collapsible sidebar category &#x27;DevOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/digital-garden/Machine Learning">Machine Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/digital-garden/arduino">arduino</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/digital-garden/digital-garden">digital-garden</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/digital-garden/m1 mac">Exploring m1 mac</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/digital-garden/philosophy/Stoicism">philosophy</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/digital-garden/ui">ui</a></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/digital-garden/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Reinforcement Learning</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Reinforcement Learning</h1></header><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Any goal can be formalized as the outcome of maximizing a cumulative reward.</p></div></div><div class="theme-admonition theme-admonition-caution alert alert--warning admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_S0QG"><p>There&#x27;s no supervision or supervised learning in RL, reward and values define the state and actions to take.</p></div></div><ul><li><p>A mapping from states to actions is called a policy.</p></li><li><p>Markov Decision Process (MDP) is a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. The probability of the next state depends only on the current state and the action taken and not on the sequence of events that preceded it. This was also taught in the Modelling and Simulation course.</p></li><li><p>Policies are stochastic, meaning that they assign probabilities to each action.</p></li><li><p>Value function has a discount factor, which is a number between 0 and 1. It is used to discount future rewards. The discount factor is used to balance the importance of immediate rewards and future rewards. A discount future of 0 means that only immediate rewards are considered. A discount factor of 1 means that all future rewards are considered equally. This led to the Bellman equation.</p></li><li><p>The value function contains the model that will predict the next action.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="multi-armed-bandit">Multi Armed Bandit<a class="hash-link" href="#multi-armed-bandit" title="Direct link to heading">​</a></h2><ul><li><p>Multi-Armed Bandit (MAB) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice&#x27;s properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to it. We wish to minimize cumulative regret and maximize cumulative reward.</p></li><li><p>The exploration-exploitation dilemma is the problem of balancing between exploiting what is already known and exploring what is not known. This is a problem in multi-armed bandits.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="greedy-policy">Greedy Policy<a class="hash-link" href="#greedy-policy" title="Direct link to heading">​</a></h3><ul><li>Greedy policy is a policy that always takes the action with the highest expected reward. It is a greedy algorithm. It is the simplest policy to implement. It is also the most naive policy. It is not always the best policy. Greedy is suboptimal because it does not consider future rewards and exploits only one action.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="epsilon-greedy-policy">Epsilon Greedy Policy<a class="hash-link" href="#epsilon-greedy-policy" title="Direct link to heading">​</a></h3><ul><li><p>Epsilon-greedy policy is a policy that takes a random action with probability epsilon and takes the greedy action with probability 1-epsilon. </p></li><li><p>Policy gradient methods are a class of reinforcement learning algorithms that can directly optimize the policy function used by the agent to select actions. They are an alternative to value-based methods such as Q-learning, which optimize a value function that estimates future rewards. Policy gradient methods are based on the idea that the policy should be updated in a direction that increases the expected total reward. </p></li><li><p>Soft max is a function that takes a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. It is used in the softmax policy.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ucb-algorithm">UCB Algorithm<a class="hash-link" href="#ucb-algorithm" title="Direct link to heading">​</a></h3><ul><li><p>UCB algorithm gives a confidence bound for each action value. The bound gets stronger as the number of times the action is selected increases. It stops selecting actions that don&#x27;t have have a good value.</p></li><li><p>Hoeffding&#x27;s inequality is an inequality in probability theory that relates the probability of the difference between the expected value of a random variable and its observed value to the variance of the random variable. It is used in the UCB algorithm.</p></li><li><p>Derivation of the UCB algorithm is <a href="https://www.youtube.com/watch?v=aQJP3Z2Ho8U&amp;list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&amp;index=2" target="_blank" rel="noopener noreferrer">here</a></p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bayesian-ucb-bucb">Bayesian UCB (BUCB)<a class="hash-link" href="#bayesian-ucb-bucb" title="Direct link to heading">​</a></h3><ul><li><p>Bayesian bandits keep track of the model&#x27;s distribution, by modelling the reward distribution as a beta distribution.</p></li><li><p>Bayesian UCB is a variant of the UCB algorithm that uses a beta distribution to model the reward distribution.</p></li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>q</mi><mrow><mo fence="true">(</mo><mi>i</mi><mo separator="true">,</mo><mi>t</mi><mo fence="true">)</mo></mrow><mo>=</mo><mi>Q</mi><mrow><mo fence="true">{</mo><mn>1</mn><mo>−</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><mo separator="true">;</mo><mi>B</mi><mi>e</mi><mi>t</mi><mi>a</mi><mrow><mo fence="true">[</mo><msub><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mrow><mo fence="true">(</mo><mi>t</mi><mo fence="true">)</mo></mrow><mo>+</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>T</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mrow><mo fence="true">(</mo><mi>t</mi><mo fence="true">)</mo></mrow><mo>−</mo><msub><mi>X</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mrow><mo fence="true">(</mo><mi>t</mi><mo fence="true">)</mo></mrow><mo>+</mo><mn>1</mn><mo fence="true">]</mo></mrow><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">q\left( i,t \right) = Q\left\{ 1 - \frac{1}{t}; Beta \left[ X_{i, j}\left( t \right) + 1, T_{i, j}\left( t \right) -X_{i, j}\left( t \right) + 1 \right] \right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">{</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal">t</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal">t</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal">t</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">1</span><span class="mclose delimcenter" style="top:0em">]</span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">}</span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi><mi>t</mi></msub><mo>=</mo><munder><mrow><mi mathvariant="normal">argmax</mi><mo>⁡</mo></mrow><mi>i</mi></munder><mi>q</mi><mrow><mo fence="true">(</mo><mi>i</mi><mo separator="true">,</mo><mi>t</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">Action_t = \operatorname*{argmax}_i q\left( i,t \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.6721em;vertical-align:-0.9221em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.1779em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm">argmax</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose delimcenter" style="top:0em">)</span></span></span></span></span></span></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="thompson-sampling">Thompson Sampling<a class="hash-link" href="#thompson-sampling" title="Direct link to heading">​</a></h3><h2 class="anchor anchorWithStickyNavbar_LWe7" id="use-of-neural-networks-in-rl">Use of Neural Networks in RL<a class="hash-link" href="#use-of-neural-networks-in-rl" title="Direct link to heading">​</a></h2><ul><li><p>When the state space is big, we can use a function approximator to approximate the value function. This is called function approximation. This allows us to use a neural network to approximate the value function. This is called a neural network approximator. A game of Go has <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>170</mn></msup></mrow><annotation encoding="application/x-tex">10^{170}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">170</span></span></span></span></span></span></span></span></span></span></span></span></span> possible states. A moving helicopter has continuous state space hence we need to use a function approximater.</p></li><li><p>RL experience is not i.i.d. because the next state depends on the current state and the action taken. </p></li><li><p>The agent policy affects the data it receives. This is due to the active nature of RL.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sota-keywords">SOTA keywords<a class="hash-link" href="#sota-keywords" title="Direct link to heading">​</a></h2><ul><li>Dueling Deep Q Networks (DDQN)</li><li>Double Deep Q Networks (DDQN)</li><li>Deep Q Networks (DQN)</li><li>Policy Gradient (PG)</li><li>Actor Critic (AC)</li><li>Advantage Actor Critic (A2C)</li><li>Asynchronous Advantage Actor Critic (A3C)</li><li>Proximal Policy Optimization (PPO)</li><li>Trust Region Policy Optimization (TRPO)</li></ul><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Atari Enduro requires a Dueling Deep Q Networks (DDQN) since the next state is undeterministic.</p></div></div><p>The traditional Q-learning model is weak since it requires access to the Q-table. Instead, we can use a Neural Network to approximate the Q-table. This is called Deep Q-Learning. The network takes in the state as input and outputs the Q-values for each action. The NN estimates the possible actions from the state provided.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="history">History<a class="hash-link" href="#history" title="Direct link to heading">​</a></h2><p>A relatively newer algorithm is Asynchronous Advantage Actor Critic (A3C). </p><ul><li>This algorithm is an extension of the Actor-Critic algorithm. </li><li>The Actor-Critic algorithm is an extension of the Policy Gradient algorithm. </li><li>The Policy Gradient algorithm is an extension of the REINFORCE algorithm. </li><li>The REINFORCE algorithm is an extension of the Monte Carlo algorithm. </li><li>The Monte Carlo algorithm is an extension of the Dynamic Programming algorithm. </li><li>The Dynamic Programming algorithm is an extension of the Bellman Equation. </li><li>According to the Bellman Equation, long-term reward in a given action is equal to the reward from the current action combined with the expected reward from the future actions taken at the following time.</li></ul><p>The Bellman Equation is given by:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>a</mi></msub><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>r</mi></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>r</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mspace linebreak="newline"></mspace><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>where, </mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>s</mi></mstyle><mtext> is the current state,</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle><mtext> is the value of the state </mtext><mstyle scriptlevel="0" displaystyle="false"><mi>s</mi></mstyle><mtext>,</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mstyle><mtext> is the next state,</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>r</mi></mstyle><mtext> is the reward,</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>a</mi></mstyle><mtext> is the action,</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>γ</mi></mstyle><mtext> is the discount factor,</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>p(s’,r|s,a) is the probability of transition, and,</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>a</mi></msub></mstyle><mtext> is the maximum value of the sum of the reward and the discounted value of the next state.</mtext></mrow></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">V(s) = max_a \sum_{s&#x27;,r} p(s&#x27;,r|s,a)[r + \gamma V(s&#x27;)] \newline \begin{align*} &amp; \textnormal{where, } \\ &amp; \textnormal{$s$ is the current state,} \\ &amp; \textnormal{$V(s)$ is the value of the state $s$,} \\ &amp; \textnormal{$s&#x27;$ is the next state,} \\ &amp; \textnormal{$r$ is the reward,} \\ &amp; \textnormal{$a$ is the action,} \\ &amp; \textnormal{$\gamma$ is the discount factor,} \\ &amp; \textnormal{p(s&#x27;,r|s,a) is the probability of transition, and,}\\ &amp; \textnormal{$max_a$ is the maximum value of the sum of the reward and the discounted value of the next state.} \\ \end{align*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4801em;vertical-align:-1.4301em"></span><span class="mord mathnormal">ma</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.856em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4301em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0519em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">γV</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)]</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:13.5em;vertical-align:-6.5em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:7em"><span style="top:-9em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:-7.5em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:-6em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:-4.5em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:-3em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:-1.5em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:0em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:1.5em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span><span style="top:3em"><span class="pstrut" style="height:2.84em"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:6.5em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:7em"><span style="top:-9.16em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord textrm">where, </span></span></span></span><span style="top:-7.66em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord mathnormal">s</span><span class="mord textrm"> is the current state,</span></span></span></span><span style="top:-6.16em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mord textrm"> is the value of the state </span><span class="mord mathnormal">s</span><span class="mord textrm">,</span></span></span></span><span style="top:-4.66em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord textrm"> is the next state,</span></span></span></span><span style="top:-3.16em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord textrm"> is the reward,</span></span></span></span><span style="top:-1.66em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord mathnormal">a</span><span class="mord textrm"> is the action,</span></span></span></span><span style="top:-0.16em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mord textrm"> is the discount factor,</span></span></span></span><span style="top:1.34em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord textrm">p(s’,r|s,a) is the probability of transition, and,</span></span></span></span><span style="top:2.84em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord mathnormal">ma</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord textrm"> is the maximum value of the sum of the reward and the discounted value of the next state.</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:6.5em"><span></span></span></span></span></span></span></span></span></span></span></span></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications">Applications<a class="hash-link" href="#applications" title="Direct link to heading">​</a></h2><p>Intelligent Traffic Control Systems is an application of Reinforcement Learning. The goal is to maximize the throughput of the traffic system. <a href="https://www.eclipse.org/sumo/" target="_blank" rel="noopener noreferrer">SUMO</a> is a popular simulator for ITCS.</p><p>Cognitive radio networks is another application of Reinforcement Learning. The goal is to maximize the throughput of the network while minimizing the interference.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="toy-examples">Toy Examples<a class="hash-link" href="#toy-examples" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cartpole-problem"><a href="/digital-garden/Reinforcement Learning/Cartpole Problem">Cartpole Problem</a><a class="hash-link" href="#cartpole-problem" title="Direct link to heading">​</a></h3><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><ul><li><a href="https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet/cheatsheet.pdf" target="_blank" rel="noopener noreferrer">Good cheatsheet on Bellman Equation</a></li><li><a href="https://web.stanford.edu/class/cs234/CS234Win2019/slides/lecture12_postclass.pdf" target="_blank" rel="noopener noreferrer">Lecture Notes on MDP and Bayesian Bandits</a></li><li><a href="https://emiliekaufmann.github.io/BIP241013.pdf" target="_blank" rel="noopener noreferrer">Lecture Notes on Bayesian and Frequentist Methods in Bandit Models</a></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2022-12-04T09:55:39.000Z">Dec 4, 2022</time></b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/digital-garden/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Hello World!</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/digital-garden/Reinforcement Learning/Cartpole Problem"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Cartpole Problem</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#multi-armed-bandit" class="table-of-contents__link toc-highlight">Multi Armed Bandit</a><ul><li><a href="#greedy-policy" class="table-of-contents__link toc-highlight">Greedy Policy</a></li><li><a href="#epsilon-greedy-policy" class="table-of-contents__link toc-highlight">Epsilon Greedy Policy</a></li><li><a href="#ucb-algorithm" class="table-of-contents__link toc-highlight">UCB Algorithm</a></li><li><a href="#bayesian-ucb-bucb" class="table-of-contents__link toc-highlight">Bayesian UCB (BUCB)</a></li><li><a href="#thompson-sampling" class="table-of-contents__link toc-highlight">Thompson Sampling</a></li></ul></li><li><a href="#use-of-neural-networks-in-rl" class="table-of-contents__link toc-highlight">Use of Neural Networks in RL</a></li><li><a href="#sota-keywords" class="table-of-contents__link toc-highlight">SOTA keywords</a></li><li><a href="#history" class="table-of-contents__link toc-highlight">History</a></li><li><a href="#applications" class="table-of-contents__link toc-highlight">Applications</a></li><li><a href="#toy-examples" class="table-of-contents__link toc-highlight">Toy Examples</a><ul><li><a href="#cartpole-problem" class="table-of-contents__link toc-highlight">Cartpole Problem</a></li></ul></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div>
<script src="/digital-garden/assets/js/runtime~main.87528bd9.js"></script>
<script src="/digital-garden/assets/js/main.dcaed46e.js"></script>
</body>
</html>